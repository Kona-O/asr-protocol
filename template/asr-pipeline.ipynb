{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import topiary\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\">BROKEN</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 0. Project organization\n",
    "\n",
    "Keeping track of all of the files is always annoying in these projects.  I'm not sure there is a \"right\" way to do it, but here are a few principles that have kept me sane over the years:\n",
    "\n",
    "+ Write up a `README` file and save it in the directory where you do your work.  This should say what you do at each step, the date you did it, and what files were involved.  This is the lab notebook for the project.  I prefer to keep this as a standalone file rather than a jupyter notebook because a text file will *always* be readable; who knows how jupyter will change in the future.\n",
    "+ Whenever you have to save out a file, do it with a numbered prefix and description.  Something like `00_initial-sequence-to-blast.fasta`, `01_blast-results.xml`, etc. This way, if you sort the directory, you can see what you did, in what order. \n",
    "+ Keep your sequences in a `.csv` file with columns for species, database id, raw sequence, aligned sequence, etc. You can then share this `.csv` file as the supplement to your paper.  *This protocol implements this `.csv` strategy throughout.*\n",
    "+ One of the standard phylogenetics file formats (`.phy`) requires sequences have 10 character names. This is *very annoying.* To deal with this problem, the pipeline assigns every sequence you add a unique 10-character ID of the format `XXXXXXXXXX` where `n` indicates an integer. At various points in the pipeline, you can interconvert between these internal sequence names and various human-readable names. \n",
    "\n",
    "PARADIGM\n",
    "```\n",
    "df = do_something(df)\n",
    "df.to_csv(\"dataset.csv\",index=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "## 1. Create a set of BLAST .xml files containing sequences\n",
    "\n",
    "Building sequence datasets is complicated.  How you go about it depends on what your goals are for the project.  In this protocol, I am going to assume we are looking members of protein families (meaning a mixture of orthologs and paralogs) from animals (meaning minimal lateral gene transfer). \n",
    "\n",
    "I'll mention two questions that often come up.  The first is: *should I include paralogs and orthologs?* For a gene tree—what we normally generate—you generally want to a large number of orthologs and paralogs. A good check for the quality of the tree is whether the orthologs group together and roughly reproduce the species tree.  In practice, this means BLASTing away and then grabbing as many sequences as possible without worrying too much about whether you are pulling down orthologs or paralogs.  \n",
    "\n",
    "One reason this is useful is that sequence databases sometimes have paralogs and orthologs mislabeled.  Imagine you are building a tree of protein *X*.  You BLAST the NCBI and pull out out a sequence labeled protein *Y*.  You then build a tree, including this protein, and find it groups squarely with the *X* proteins, but not the other *Y* proteins in your dataset.  The simplest explanation for this result is that the protein was annotated incorrectly.   If you had dropped this sequence from your analysis, just because it was labeled *Y*, you would have removed valuable information from your dataset.  \n",
    "\n",
    "A second question: *should I include \"hypothetical,\" predicted\", and \"low-quality\" sequences?* As is usual for bioinformatics, the answer is \"it depends.\" If the protein has a huge number of isoforms and weirdo splice sites, the predicted genes could be difficult to align and thus mess up your phylogeny. On the other hand, if the protein has a very well defined set of exons, you'd probably be okay. Further, you sometimes really need sequences from taxa for which sequencing data are particularly poor and a \"low-quality\" sequence is the best you can do.  In the following pipeline, we include everything up front, and then remove redundant sequences.  We preferentially remove these low-quality sequences if better sequences are available. This is implemented in the jupyter notebooks included in this protocol. \n",
    "\n",
    "### Practical thoughts:\n",
    "\n",
    "+ The bigger the initial dataset, the better. We can pare down later. \n",
    "\n",
    "+ Do not worry about duplicate sequences at this point; we'll remove them later. \n",
    "\n",
    "+ BLAST with multiple paralogs for the gene of interest, sampled from multiple species. \n",
    "\n",
    "+ Use PSI-BLAST to iteratively build your dataset.  In PSI-BLAST, your results from the first BLAST search are fed into the next BLAST search, yielding more hits overall.  This process can be repeated until you find no more new hits. \n",
    "\n",
    "+ Check for good taxonomic sampling. (You gain more information from 1,000 sequences from across mammals than just taking 1,000 sequences from rodents.) The `Taxonomy` tab on the blast interface is useful for this. If you are not seeing sequences from key species, you can do targeted BLAST within a clade.  \n",
    "\n",
    "  + To do this, use the `Organism` selection flag to make sure you get sequences from key lineages.  Since we often study vertebrate proteins, here's a useful strategy: do separate BLAST searches for: `Mammalia (taxid:40674)`, `Sauropsida (taxid:8457)`, `Amphibia (taxid:8292)`,  `bony fishes  (taxid:7898)`,  `Elasmobranchii (taxid:7778)` (skates, rays and sharks), `jawless vertebrates (taxid:1476529)` (hagfish and lampreys), and `tunicates (taxid:7712)` (closest non-vertebrate outgroup). I also usually do `lobe-finned fishes (taxid:118072)`, excluding `Tetrapoda (taxid:32523)`.  This should pull up lungfish and coelacanth sequences if available. \n",
    "  + If you get a ton of hits in your earliest-diverging lineage--`tunicates` above--it suggests the protein evolved earlier than you have sampled.  If so, expand to earlier-diverging groups.  In this case, you would expand to earlier-diverging groups like `Ecdysozoa (taxid:1206794)` (which includes both *D. Melanogaster* and *C. elegans*), `Lophotrochozoa (taxid:1206795)` and `Hemichordata (taxid:10219)`. \n",
    "  + If one of those searches yields a small number of hits, it might be worthwhile to search for non-NCBI genomes and transcriptomes in an effort to fill out the taxa. Some lineages are just have few sequence resources.  Amphibians are notoriously undersampled, and there are very few extant jawless vertebrates. As a result, these bits of vertebrate protein/gene treees are often sparse. \n",
    "  \n",
    "### Other sources of sequences\n",
    "\n",
    "*Note: If you have to go this route, you'll have to manually load the non-NCBI results into an existing topiary dataframe in excel or the like. See #10 XX below for an example.*\n",
    "\n",
    "+ Some good places to look for sequences outside of NCBI are [Fish10K](https://db.cngb.org/datamart/animal/DATAani16/) and [Bird10K](https://b10k.genomics.cn/index.html). You can also switch to [tblastn](https://blast.ncbi.nlm.nih.gov/Blast.cgi?PROGRAM=tblastn&PAGE_TYPE=BlastSearch&LINK_LOC=blasthome) approach to see if there are nucleic acid sequences corresponding to your protein that have not been annotated as proteins.  I find [https://ensembl.org/](https://ensembl.org/) is the easiest database for this task. Finally, if you are desperate, can look for transcriptomes in the [short read archive](https://www.ncbi.nlm.nih.gov/sra).  These often have to be assembled (a somewhat tedious process). \n",
    "\n",
    "+ [PFAM](http://pfam.xfam.org/) has some amazing pre-built alignments.  I've generally found them more useful for studies of whole domains evolving over very long timescales than our typical vertebrate protein work, but it's worth keeping in mind these alignments are out there. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load sequences from xml files into a dataframe\n",
    "\n",
    "This will create a data frame and .csv file with all of the sequences you identified by BLAST.  Most of the columns are self-explanatory, but there are two important columns that bear more explanation: \n",
    " + `uid`: a random 10-letter string unique to each sequence, used for generating files compatible with PAML. \n",
    " + `keep`: whether or not the sequence should be written out in alignments. Sequences are never deleted from the dataframe, just marked as `keep = False`. \n",
    " \n",
    "Running this code requires you provide a list of `.xml` files.  You can also "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of xml files to load and process\n",
    "list_of_xml_files = [\"../data/amphibian.xml\"] #,\"example/mammalian.xml\",\"example/sauropsid.xml\"]\n",
    "\n",
    "# Aliases as a dictionary. It will map anything in the values to the key. For\n",
    "# this example, \"lymphocyte antigen 96\", \"MD2\", and \"MD-2\" will all be replaced\n",
    "# by \"LY96\". \n",
    "alias_dictionary = {\"LY96\":(\"lymphocyte antigen 96\",\"MD2\",\"MD-2\"),\n",
    "                    \"LY86\":(\"lymphocyte antigen 86\",\"MD1\",\"MD-1\")}\n",
    "\n",
    "# Load sequences into data frame\n",
    "df = topiary.ncbi_blast_xml_to_df(list_of_xml_files,aliases=alias_dictionary)\n",
    "\n",
    "# Write output file\n",
    "df.to_csv(\"01_initial-hits.csv\",index=False)\n",
    "\n",
    "# Print to notebook\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Check sequence identities using reverse BLAST\n",
    "\n",
    "BLAST can pull down sequences that are homologous, but outside the clade of interest.  (For example, we might want to study TLR4, but BLAST also pulls up TLR2). To identify these sequences, we use reverse BLAST each sequence against the human genome. We will keep only those sequences that pull up the proteins of interest from the human genome. \n",
    "\n",
    "The code below will do this reverse BLAST, adding three columns to the data frame: \n",
    " + `rev_hit`: the definition of the top reverse BLAST hit. \n",
    " + `paralog`: the paralog call based on the reverse BLAST\n",
    " + `hit_ratio`: the ratio of the e-value for the hit and its next-best hit. \n",
    " \n",
    "It will also update `keep` to `False` if no reverse hit was found.\n",
    "\n",
    "`call_dict` determines how calls are made. `rev_blast_db` determines what local database to use for the reverse BLAST.  \n",
    "\n",
    "XX UPDATE WITH NEW REV_SECOND_RATIO BIT ... FIX THAT CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data frame from the previously written file.  This is not necessary\n",
    "# if you are running the notebook in order, but is super handy if you want to \n",
    "# start the notebook midway through the analysis. \n",
    "df = pd.read_csv(\"01_initial-hits.csv\")\n",
    "\n",
    "# Perform reverse blast, looking for hits on \"lymphocyte antigen 96\" and \n",
    "# \"lymphocyte antigen 86\" from the human genome, labeling them as LY96 \n",
    "# and LY86 respectively. \n",
    "df = topiary.reverse_blast(df,\n",
    "                           call_dict={\"lymphocyte antigen 96\":\"LY96\",\n",
    "                                      \"lymphocyte antigen 86\":\"LY86\"},\n",
    "                           rev_blast_db=\"GRCh38\")\n",
    "\n",
    "# Write output file\n",
    "df.to_csv(\"02_reverse-blasted.csv\",index=False)\n",
    "\n",
    "# Print to notebook\n",
    "df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4. Find unique species identifiers\n",
    "\n",
    "Idea here is to make sure we actually know what species each sequence comes from. NCBI and the opentree of life use slightly different sequence names. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"02_reverse-blasted.csv\")\n",
    "df = topiary.get_ott_id(df,context_name=\"Animals\")\n",
    "df[df.ott.isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In these example data, \"Apteryx mantelli mantelli\" is not found in opentree of life.  If you start typing the species name into the open tree of life search engine (https://tree.opentreeoflife.org/), it pops up \"Apteryx australis mantelli\".  A quick google search reveals these are the same species. You can update this by the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df.species == \"Apteryx mantelli mantelli\",\"species\"] = \"Apteryx australis mantelli\"\n",
    "df.loc[df.species == \"Apteryx australis mantelli\",\"keep\"] = True\n",
    "df = topiary.get_ott_id(df,context_name=\"Animals\")\n",
    "df[df.ott.isnull()]\n",
    "\n",
    "df.to_csv(\"02.1_with-ott.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 5. Lower redundancy of sequences\n",
    "\n",
    "To make the computation faster and avoid bias from inclusion of many very similar sequences, we usually remove sequences that are highly similar to one another. The code below will combine sequences with identities greater than 0.9, using relatively intelligent criteria to choose the higher quality sequence. \n",
    "\n",
    "The `key_species` list is a list of species that will be given preference over others. If we specify `[\"Homo sapiens\",\"Mus musculus\"]` as key species and then find a human and chimp sequence are highly similar, the software will drop the chimp. The software does two loops.  In the first loop, it discards similar sequences within each species. This *will* drop sequences from key species. (If you had two human sequences 99% identical, it would drop the lower quality sequence).  In the second loop, the software discards similar sequences between species.  That loop *will not* drop sequences from key species.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data frame from the previously written file.  This is not necessary\n",
    "# if you are running the notebook in order, but is super handy if you want to \n",
    "# start the notebook midway through the analysis. \n",
    "df = pd.read_csv(\"02.1_with-ott.csv\")\n",
    "\n",
    "# Remove redundancy\n",
    "key_species = [\"Homo sapiens\",\"Mus musculus\",\"Monodelphis domestica\",\"Gallus gallus\",\n",
    "               \"Xenopus laevis\",\"Danio rerio\"]\n",
    "df = topiary.remove_redundancy(df,0.90,key_species=key_species)\n",
    "\n",
    "# Write out file\n",
    "df.to_csv(\"03_removed-redundancy.csv\",index=False)\n",
    "\n",
    "# Print in notebook\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 6. Check and edit reduced data frame\n",
    "\n",
    "At this point, you might want to look over the set of sequences and see if you like the result of the automatic redundancy reduction.  Some things to look for:\n",
    "\n",
    "+ Is your sequence set still huge (>1000) or too small (<100)? (If so, you might want to play with the redundnacy cutoff above). \n",
    "+ Do you still have the sequence of modern species you care about? (If so, you may want to manually set those sequences to `keep = True`). \n",
    "\n",
    "You could load up the nonredudnant set in excel, but I *strongly* recommend you do these manipulations using the pandas dataframe. Pandas slicing allows you to easily pull out rows you care about based on selection criteria. The cell below shows a few of these slicing approaches as templates. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many sequences are there in the dataset?\n",
    "print(np.sum(df.keep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many of each of the key species (defined above) are in the dataset?\n",
    "for k in key_species:\n",
    "    print(k,np.sum(np.logical_and(df.keep,df.species==k)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show all of the human sequences we're keeping\n",
    "df[np.logical_and(df.keep,df.species==\"Homo sapiens\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Align the sequences using MUSCLE\n",
    "\n",
    "We now have a database of sequences.  We now need to align those sequences to one another.  The code below will create a file called `04_to-align.fasta` that has all of the sequences flagged with `keep = True`. The sequences will be assigned \"pretty\" names that have a defined structure: `ortholog_call|species|accession`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data frame from the previously written file.  This is not necessary\n",
    "# if you are running the notebook in order, but is super handy if you want to \n",
    "# start the notebook midway through the analysis. \n",
    "df = pd.read_csv(\"03_removed-redundancy.csv\")\n",
    "\n",
    "# Write fasta file. \n",
    "topiary.write_fasta(df,\"04_to-align.fasta\",seq_name=\"pretty\")\n",
    "\n",
    "# Align the sequences in 04_to-align and create 05_aligned\n",
    "from Bio.Align.Applications import MuscleCommandline\n",
    "cmd = MuscleCommandline(input=\"04_to-align.fasta\", out=\"05_aligned.fasta\")\n",
    "stdout, stderr = cmd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Manually edit the alignment using AliView\n",
    "\n",
    "XX NOTE THIS IS ASR, SO SLIGHTLY DIFFERENT THAN OTHER TREE INFERENCE PROBLEMS XX\n",
    "Human brains are still better than computers at identifying patterns in sequence data. We're going to edit the alignment manually. Two consequences of this:\n",
    "\n",
    "1. We need to publish our final alignment with our manuscript. It needs to be available for evaluation by readers and/or to reproduce the work. \n",
    "\n",
    "2.  If we're doing ancestral sequence reconstruction an we *delete* a column, we need to make sure we're okay with not including that column in the final reconstruction.  This probably makes sense for N- and C-terminal extensions, but makes less sense for columns in the middle of the protein. \n",
    "\n",
    "With that in mind, open the `05_aligned.fasta` file in aliview and do the following:\n",
    "\n",
    "1. <span style=\"color:blue\">*Look for sequences that are way longer or shorter than the average.*</span>  Super long sequences may align poorly--and suck other sequences into that poor alignment.  Super short sequences are also difficult to align and provide little taxonomic information.  Delete these sequences by selecting them and going to `Edit->Delete selected`.  When you reimport the alignment into your dataframe, the software will set `keep = False` for any sequence you deleted. \n",
    "\n",
    "2. <span style=\"color:blue\">*Trim random long N-terminal and C-terminal extensions from alignment.*</span>  Deleting sequences that align poorly will not bias your tree, but including incorrectly aligned regions might. Select these sequence regions and go to `Edit->Clear selected bases`. \n",
    "\n",
    "3. <span style=\"color:blue\">*Manually realign problematic regions.*</span>  Sometimes, alignment programs will make obvious mistakes, where the same sequence element is aligned different ways right next to one another.  To correct for this, you can manually move groups of amino acids by selecting them and dragging them right or left. You can also select whole blocks of the alignment and then go to `Align->Realign selected block` to re-run MUSCLE on that block. \n",
    "\n",
    "4. <span style=\"color:blue\">*Remove empty columns and rows.*</span> Remove gaps-only columns (`Edit->Delete gap-only columns`) and any empty sequences (`Edit->Delete empty sequences`). \n",
    "\n",
    "5. <span style=\"color:blue\">*Save out the edited alignment*</span> as:\n",
    "```\n",
    "06_aligned-edited.fasta\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Load newly aligned sequences into our dataframe and write out .phy file for tree building\n",
    "\n",
    "We will now load our alignment back into our dataframe.  This will create a new column called \"alignment\" with the aligned sequences and will also set all sequences *not* in the alignment file to have \"keep = False\".  We will then write out the aligned sequences into a .phy file, the file format used by RAxML and other tree-inference software packages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data frame from the previously written file.  This is not necessary\n",
    "# if you are running the notebook in order, but is super handy if you want to \n",
    "# start the notebook midway through the analysis. \n",
    "df = pd.read_csv(\"03_removed-redundancy.csv\")\n",
    "\n",
    "# Load the alignment into the data frame\n",
    "df = topiary.load_fasta(df,\"06_aligned-edited.fasta\",load_into_column=\"alignment\")\n",
    "\n",
    "# Write out file\n",
    "df.to_csv(\"07_seq-database.csv\",index=False)\n",
    "\n",
    "# Print df to notebook\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"07_seq-database.csv\")\n",
    "t = topiary.get_species_tree(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Write out the alignment in .phy format\n",
    "topiary.write_phy(df,\"08_for-tree-building.phy\",seq_column=\"alignment\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 9. Generate ML phylogenetic tree using RAxML\n",
    "\n",
    "Our next steps are to find a good evolutionary model that describes our data and to build a maximum likelihoo phylogenetic tree.  This is likely something you will want to run on a high-performance computing cluster.  You need to copy \"08_for-tree-building.phy\" and \"raxml.py\" to whatever server you use. Hopefully it already has \"raxmlHPC\" installed.  You then need to execute two commands, using the helper `raxml.py` script.\n",
    "\n",
    "The first will search through a collection of different models of evolutionary rate distribution, amino acid substitution probability, etc. and find the model that gives the highest likelihood.  \n",
    "\n",
    "```\n",
    "./raxml.py model -a 08_for-tree-building.phy -o 09_find-model\n",
    "```\n",
    "\n",
    "This will print out the best model at the end, along with an `AIC Prob` score.  Hopefully, that number is close to 1.0, meaning that the chosen model is clearly a better choice than all the others. (If not, we may need to reconstruct our ancestors using different models to make sure the results are robust to the choice of model).  If you want to see the likelihoods and AIC probabilities for all models, check out `find_model/model-comparison.csv`). \n",
    "\n",
    "We next need to build the maximum likelihood phylogenetic tree for our alignment. To do that, run the following.  It will automatically grab the best model from the previous calculation. \n",
    "```\n",
    "./raxml.py ml -a 08_for-tree-building.phy -o 10_ml-tree -m `cat 09_find-model/best-model.txt`\n",
    "```\n",
    "\n",
    "Once complete, you can download the resulting `09_find-model` and `10_ml-tree` directories to your local computer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topiary.reconcile.create_generax(df,\"09_ml-tree/02_ml-tree.newick\",\"LG+G8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 10. Evaluate tree\n",
    "\n",
    "The next step is to look at the ML tree and make sure it is well supported/sensical. To do so, you should first write out the tree with useful names for each taxon. To do so, run the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data frame from the previously written file.  This is not necessary\n",
    "# if you are running the notebook in order, but is super handy if you want to \n",
    "# start the notebook midway through the analysis. \n",
    "df = pd.read_csv(\"07_seq-database.csv\")\n",
    "\n",
    "# Make the sequence names on the output tree human readable. \n",
    "topiary.util.uid_to_pretty(df,\"10_ml-tree/07_final-tree.newick\",out_file=\"11_ml-tree.newick\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open `11_ml-tree.newick` in FigTree.  On the left-hand panel, go to \"Branch-labels\" and select \"Display: label\". This will label each branch with its SH support.  SH support values go from 0 (no support at all) to 100 (excellent support).  Then look at the following:\n",
    "\n",
    "+ *Are the major clades well supported?* Major branch points should (hopefully) have $SH \\ge 85$. If not, we may need to do our reconstructions on multiple versions of the tree to see if our ancestral sequences are robust to the tree topology. \n",
    "+ *Is the species tree approximately correct?* Do you see birds with birds, mammals with mammals, etc.?   If not, there could be a problem with the current alignment, or we might need to add more sequences to the alignment. \n",
    "+ *Are there long branches?* A long branch is one where you have a bunch of sequence change (say 0.7 subs/site) without branching.  This means the evolutionary model runs and runs without getting input from branches.  This can lead to bias and will certainly lead to very poor reconstructions of ancestors near the long branch. If there is a long branch for a single sequence, delete it from the alignment. It is too divergent or too poorly aligned to include effectively. If a long branch occurs between clades, you can try to find new sequences that \"break\" the branch.  For example, if there is a long branch between bony fishes and birds, adding amphibian sequences will cut the branch (about) in half and should improve the inference. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Iteratively add sequences to alignment\n",
    "\n",
    "At this point, you may need to go back and add sequences to the alignment. To do so, you have a couple of options.  One possibility is to open `07_seq-database.csv` in excel and manually add any new sequences to the database. Make sure you fill out every column, including pasting the sequence into the `alignment` column. You could also add new rows via pandas (see #5 for some examples). \n",
    "\n",
    "After you've edited the sequence database, save it out as `12_seq-database.csv`. Write this out as a fasta file.  You can then load into aliview, edit, and repeat steps 6-9 until you are satisfied with the ML tree.  (The following code block is an example of what you might run in a jupyter notebook.) \n",
    "\n",
    "```\n",
    "# Read in the manually edited sequence database\n",
    "df = pd.read_csv(\"12_seq-database.csv\")\n",
    "\n",
    "# Write out a fasta file. \n",
    "topiary.write_fasta(df,\"13_new-alignment.fasta\",seq_name=\"pretty\",seq_column=\"alignment\")\n",
    "\n",
    "### edit in alivew and save as 14_aligned-edited.fasta\n",
    "\n",
    "# Load the alignment into the data frame\n",
    "df = topiary.load_fasta(df,\"14_aligned-edited.fasta\",load_into_column=\"alignment\")\n",
    "\n",
    "# Write out file\n",
    "df.to_csv(\"15_seq-database.csv\",index=False)\n",
    "\n",
    "# Write out the alignment in .phy format\n",
    "topiary.write_phy(df,\"16_for-tree-building.phy\",seq_column=\"alignment\")\n",
    "\n",
    "```\n",
    "\n",
    "Copy `16_for-tree-building.phy` to your favorite cluster and use it to calculate a new maximum likelihood tree. \n",
    "\n",
    "```\n",
    "./raxml.py ml -a 16_for-tree-building.phy -o 17_ml-tree -m `cat 09_find-model/best-model.txt`\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Assign paralogs\n",
    "\n",
    "Once you have a tree and alignment that you are happy with, you now need to identify which sequence corresponds to which paralog. The reverse BLAST protocol we used above is error-prone, particularly for highly diverged sequences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data frame from the previously written file.  This is not necessary\n",
    "# if you are running the notebook in order, but is super handy if you want to \n",
    "# start the notebook midway through the analysis. \n",
    "df = pd.read_csv(\"07_seq-database.csv\")\n",
    "\n",
    "df.loc[df.accession == \"OCT56128.1\",\"paralog\"] = \"LY96a\"\n",
    "df.loc[df.accession == \"XP_018120758.1\",\"paralog\"] = \"LY96b\"\n",
    "df.loc[df.accession == \"XP_040288185.1\",\"paralog\"] = \"LY86b\"\n",
    "df.loc[df.accession == \"XP_029446462.1\",\"paralog\"] = \"LY86b\"\n",
    "df.loc[df.accession == \"XP_029447354.1\",\"paralog\"] = \"LY86a\"\n",
    "df.loc[df.accession == \"XP_029446462.1\",\"paralog\"] = \"LY86b\"\n",
    "df.loc[df.accession == \"CAF5201687.1\",\"paralog\"] = \"LY96a\"\n",
    "df.loc[df.accession == \"XP_033791070.1\",\"paralog\"] = \"LY96a\"\n",
    "df.loc[df.accession == \"XP_040288186.1\",\"paralog\"] = \"LY86a\"\n",
    "\n",
    "df.to_csv(\"12_seq-database.csv\")\n",
    "\n",
    "\n",
    "# Make the sequence names on the output tree human readable. \n",
    "#topiary.util.uid_to_pretty(\"17_ml-tree/final-tree.newick\",\"18_ml-tree.newick\",df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, load up `18_ml-tree.newick` in FigTree. Look for well-supported clades that contain sequences with known paralogy and make sure all of the sequences in that clade have the same paralog name.  For example, if you have a clade with SH = 100 that contains human S100A9, but also a pangolin protein labeled S100A8, the pangolin protein is labeled incorrectly. \n",
    "\n",
    "Either via pandas or excel, edit the `paralog` column of your sequence database with the correct call for each ortholog. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Generate species tree(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"12_seq-database.csv\")\n",
    "df = topiary.get_ott_id(df,context_name=\"Animals\")\n",
    "species_tree  = topiary.get_species_tree(df)\n",
    "\n",
    "gene_tree = \"((LY96a,LY96b),(LY86a,LY86b));\"\n",
    "\n",
    "#possible_paralogs = [\"A9\",\"A8\",\"A12\",\"MRP126\"]\n",
    "#df[\"paralog\"] = np.random.choice(possible_paralogs,len(df))\n",
    "\n",
    "final_tree = topiary.build_species_corrected_gene_tree(df,species_tree,gene_tree)\n",
    "\n",
    "final_tree.write_to_path(\"13_species-tree.newick\",schema=\"newick\")\n",
    "topiary.write_phy(df,\"14_final-alignment.phy\",seq_column=\"alignment\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Reconstruct ancestors on tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "```\n",
    "./raxml.py asr -a final-alignment.phy -m PROTXXX -t final-tree.newick -o ancestors\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
